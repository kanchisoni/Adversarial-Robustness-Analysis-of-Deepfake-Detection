# Adversarial-Robustness-Analysis-of-Deepfake-Detection
This project analyzes the adversarial robustness of a CNN-based deepfake detection system. FGSM attacks are used to generate adversarial samples, causing severe accuracy drops. Frequency-domain and preprocessing defenses such as Gaussian blur and wavelet transforms are applied to improve robustness without retraining the model.
